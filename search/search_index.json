{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HTC-Grid \u00b6 The high throughput compute grid project (HTC-Grid) is a container based cloud native HPC/Grid environment. The project provides a reference architecture that can be used to build and adapt a modern High throughput compute solution using underlying AWS services, allowing users to submit high volumes of short and long running tasks and scaling environments dynamically. Warning : This project is an Open Source (Apache 2.0 License), not a supported AWS Service offering. When should I use HTC-Grid ? \u00b6 HTC-Grid should be used when the following criteria are meet: high task throughput is required (from 250 to 10,000+ tasks per second). The tasks are loosely coupled. Variable workloads (tasks with heterogeneous execution times) are expected and the solution needs to dynamically scale with the load. When should I not use the HTC-Grid ? \u00b6 HTC-Grid might not be the best choice if : The required task throughput is below 250 tasks per second: Use AWS Batch instead. The tasks are tightly coupled, or use MPI. Consider using either AWS Parallel Cluster or AWS Batch Multi-Node workloads instead The tasks uses third party licensed software. How do I use HTC-Grid ? \u00b6 If you want to use the HTC-Grid, please visit the following pages: Getting Started User Guide Developer Guide The workshop","title":"HTC-Grid"},{"location":"#htc-grid","text":"The high throughput compute grid project (HTC-Grid) is a container based cloud native HPC/Grid environment. The project provides a reference architecture that can be used to build and adapt a modern High throughput compute solution using underlying AWS services, allowing users to submit high volumes of short and long running tasks and scaling environments dynamically. Warning : This project is an Open Source (Apache 2.0 License), not a supported AWS Service offering.","title":"HTC-Grid"},{"location":"#when-should-i-use-htc-grid","text":"HTC-Grid should be used when the following criteria are meet: high task throughput is required (from 250 to 10,000+ tasks per second). The tasks are loosely coupled. Variable workloads (tasks with heterogeneous execution times) are expected and the solution needs to dynamically scale with the load.","title":"When should I use HTC-Grid ?"},{"location":"#when-should-i-not-use-the-htc-grid","text":"HTC-Grid might not be the best choice if : The required task throughput is below 250 tasks per second: Use AWS Batch instead. The tasks are tightly coupled, or use MPI. Consider using either AWS Parallel Cluster or AWS Batch Multi-Node workloads instead The tasks uses third party licensed software.","title":"When should I not use the HTC-Grid ?"},{"location":"#how-do-i-use-htc-grid","text":"If you want to use the HTC-Grid, please visit the following pages: Getting Started User Guide Developer Guide The workshop","title":"How do I use HTC-Grid ?"},{"location":"api/","text":"API documentation \u00b6 This section outlines how to develop and deploy a custom application on HTC-Grid. At the top level, there are 3 main components that need to be developed: A client application(s) that will interact with a deployment of HTC-Grid by submitting tasks and retrieving results. A worker (lambda) function that will be receiving and executing tasks. Configuration of the HTC-Grid's deployment process to incorporate all the relevant changes (specifically for the backhand worker functions).","title":"API documentation"},{"location":"api/#api-documentation","text":"This section outlines how to develop and deploy a custom application on HTC-Grid. At the top level, there are 3 main components that need to be developed: A client application(s) that will interact with a deployment of HTC-Grid by submitting tasks and retrieving results. A worker (lambda) function that will be receiving and executing tasks. Configuration of the HTC-Grid's deployment process to incorporate all the relevant changes (specifically for the backhand worker functions).","title":"API documentation"},{"location":"api/client/","text":"Client API (between application and HTC-Grid) \u00b6 HTC-Grid provides a Python3 API (please refer to API Reference for more details) with several client application examples of how to use the API (see client.py , cancel_tasks.py , and portfolio_pricing_client.py examples in /examples/client/python ). These examples show how to: - Connect to an HTC-Grid deployment - Authenticate the client - Submit multiple tasks - Wait for the results - Retrieve results Follow these examples to write a custom applications.","title":"Client API (between application and HTC-Grid)"},{"location":"api/client/#client-api-between-application-and-htc-grid","text":"HTC-Grid provides a Python3 API (please refer to API Reference for more details) with several client application examples of how to use the API (see client.py , cancel_tasks.py , and portfolio_pricing_client.py examples in /examples/client/python ). These examples show how to: - Connect to an HTC-Grid deployment - Authenticate the client - Submit multiple tasks - Wait for the results - Retrieve results Follow these examples to write a custom applications.","title":"Client API (between application and HTC-Grid)"},{"location":"api/client/python/reference/","text":"HTC-Grid API Reference \u00b6 Client API (Python) \u00b6 This section outlines how client application can connect and interact with a deployed HTC-Grid. AWSConnector is the main class responsible for communication with the HTC-Grid. Upon creating the class api . connector . AWSConnector from api.connector import AWSConnector These are the available methods: AWSConnector authenticate send get_results cancel_sessions Constructor - AWSConnector \u00b6 Request Syntax gridConnector = AWSConnector ( client_config_data = { 'grid_storage_service' : 'REDIS' | 'S3' , 's3_bucket' : 'string' , 'redis_url' : 'string' , 'public_api_gateway_url' : 'string' , 'private_api_gateway_url' : 'string' , 'api_gateway_key' : 'string' , 'user_pool_id' : 'string' , 'cognito_userpool_client_id' : 'string' , 'username' : 'string' , 'password' : 'string' , 'dynamodb_results_pull_interval_sec' : 'number' , 'task_input_passed_via_external_storage' : 'number' , 'region' : 'string' } ) Parameters client_config_data (dict) [REQUIRED] grid_storage_service - Determines which storage will be used for Data Plane 'REDIS'|'S3' s3_bucket - The name of the S3 bucket that is used as a back-end for Data Plane redis_url - The URL of the Redis deployment that is used as a back-end for the Data Plane public_api_gateway_url - private_api_gateway_url - api_gateway_key - user_pool_id - cognito_userpool_client_id - username - (optional) username for Cognito userpool, if the field is not present, username property is read from environment variable USERNAME password - (optional) password for Cognito userpool, if the field is not present, password property is read from environment variable PASSWORD dynamodb_results_pull_interval_sec - The frequency that the client uses to fetch results from DynamoDB. REGION - Region where HTC-Grid is deployed Return type AWSConnector Object Method - authenticate \u00b6 There are currently three ways to authenticate a client. 1. Passing username and password via client_config_data when initializing AWSConnector (not recommended). 2. Setting username and password in the environmental variables 3. If client application and HTC-Grid are located in the same VPN then there is no need for explicit authentication. However, an additional environmental variable needs to be set INTRA_VPC=1 allowing AWSConnector to skip username and password. Request Syntax gridConnector . authenticate () Parameters None Return type None Returns None Method - send \u00b6 This function is used to send task(s) to the HTC-Grid. Request Syntax gridConnector . send ( tasks_list = [ {}, ] ) Parameters tasks_list (list) [REQUIRED] A list of serializable dictionaries. Each dictionary will be passed to the execution lambda function as an event argument. Each dictionary will be encoded to base 64 before being stored in the Data Plane and then decoded before being passed to the execution lambda function. Output produced by the lambda function will be passed the same way in the reverse direction. Encoding and decoding is done by the grid connector, client only needs to provide serializable dictionary as input and output of the lambda functions. input : base64 . b64encode ( json . dumps ( input_dict ) . encode ( 'utf-8' )) output : base64 . b64decode ( output_dict ) . decode ( 'utf-8' ) Return type Dict Returns On successful submission, function returns a dictionary. { 'session_id' : 'string' , 'task_ids' : [ 'string' , ], } session_id - a single session ID that is associated with the submission. task_ids - an ordered list of task IDs associated with each task that was submitted in the request. Method - get_results \u00b6 Blocking function, waits until all tasks in the session are completed or until the timeout is expired. Function returns task IDs that have reached their terminal state (i.e., their states will not change). - Note , function does not return outputs of the lambdas, it is responsibility of the client to retrieve results from the Data Plane asynchronously. This function merely indicates that tasks are completed and results can be retrieved from the Data Plane. Request Syntax gridConnector . get_results ( submission_response = { 'session_id' : 'string' , 'task_ids' : [ 'string' , ], } timeout_sec = 'number' ) Parameters submission_response - a dictionary that was returned after successful submission of tasks. submission_response must contain a valid session_id and a list of associated task_ids . timeout_sec - the function will periodically (based on dynamodb_results_pull_interval_sec ) will try to pull for results until all tasks in the session are reached their terminal states or until the timeout is expires. The function uses the length of the task_ids list to determine the number of expected responses from the grid. Return type Dict Returns { 'finished' : [ 'string' , ] 'finished_OUTPUT' : [ 'string' , ], 'failed' : [ 'string' , ] 'failed_OUTPUT' : [ 'string' , ], 'cancelled' : [ 'string' , ] 'cancelled_OUTPUT' : [ 'string' , ], 'metadata' : { 'tasks_in_response' : 'number' } } * finished (optional) - list of finished task IDs * finished_OUTPUT (optional) - returns a string output produced by the lambda function * cancelled (optional) list of canceled task IDs * cancelled_OUTPUT (optional) - returns a hard-coded string read_from_REDIS indicating that actual output should be read from Data Plane, it is responsibility of the client to do that. * failed (optional) list of failed task IDs * failed_OUTPUT (optional) - returns a hard-coded string read_from_REDIS indicating that actual output should be read from Data Plane, it is responsibility of the client to do that. * metadata * tasks_in_response - total number of task in the terminal state (finished + failed + canceled) returned to the response. For example, if none of the tasks in the session have reached their terminal states an expected return is as follows: { 'metadata' : { 'tasks_in_response' : 0 } } Method - cancel_sessions \u00b6 Request Syntax response = gridConnector . cancel_sessions ( [ 'string' , ] ) Parameters Function takes list of session IDs to be canceled Return type Dict Returns Function returns dictionary of processed session IDs. { 'string' : { 'cancelled_retrying' : 0 , 'cancelled_pending' : 3 , 'cancelled_processing' : 1 , 'total_cancelled_tasks' : 4 }, 'string' : { .... }, #session - 2 'string' : { .... }, #session - 3 } string - keys of the response dictionary are session IDs that were passed in for cancellation. Sub-dictionaries contain information about how many tasks were moved into canceled state. cancelled_retying - number of tasks moved from retrying state into canceled state cancelled_pending - number of tasks moved from pending state into canceled state cancelled_processing - number of tasks moved from the processing state into canceled state. total_cancelled_tasks - total number of tasks that has been affected by this invocation.","title":"HTC-Grid API Reference"},{"location":"api/client/python/reference/#htc-grid-api-reference","text":"","title":"HTC-Grid API Reference"},{"location":"api/client/python/reference/#client-api-python","text":"This section outlines how client application can connect and interact with a deployed HTC-Grid. AWSConnector is the main class responsible for communication with the HTC-Grid. Upon creating the class api . connector . AWSConnector from api.connector import AWSConnector These are the available methods: AWSConnector authenticate send get_results cancel_sessions","title":"Client API (Python)"},{"location":"api/client/python/reference/#constructor-awsconnector","text":"Request Syntax gridConnector = AWSConnector ( client_config_data = { 'grid_storage_service' : 'REDIS' | 'S3' , 's3_bucket' : 'string' , 'redis_url' : 'string' , 'public_api_gateway_url' : 'string' , 'private_api_gateway_url' : 'string' , 'api_gateway_key' : 'string' , 'user_pool_id' : 'string' , 'cognito_userpool_client_id' : 'string' , 'username' : 'string' , 'password' : 'string' , 'dynamodb_results_pull_interval_sec' : 'number' , 'task_input_passed_via_external_storage' : 'number' , 'region' : 'string' } ) Parameters client_config_data (dict) [REQUIRED] grid_storage_service - Determines which storage will be used for Data Plane 'REDIS'|'S3' s3_bucket - The name of the S3 bucket that is used as a back-end for Data Plane redis_url - The URL of the Redis deployment that is used as a back-end for the Data Plane public_api_gateway_url - private_api_gateway_url - api_gateway_key - user_pool_id - cognito_userpool_client_id - username - (optional) username for Cognito userpool, if the field is not present, username property is read from environment variable USERNAME password - (optional) password for Cognito userpool, if the field is not present, password property is read from environment variable PASSWORD dynamodb_results_pull_interval_sec - The frequency that the client uses to fetch results from DynamoDB. REGION - Region where HTC-Grid is deployed Return type AWSConnector Object","title":"Constructor - AWSConnector"},{"location":"api/client/python/reference/#method-authenticate","text":"There are currently three ways to authenticate a client. 1. Passing username and password via client_config_data when initializing AWSConnector (not recommended). 2. Setting username and password in the environmental variables 3. If client application and HTC-Grid are located in the same VPN then there is no need for explicit authentication. However, an additional environmental variable needs to be set INTRA_VPC=1 allowing AWSConnector to skip username and password. Request Syntax gridConnector . authenticate () Parameters None Return type None Returns None","title":"Method - authenticate"},{"location":"api/client/python/reference/#method-send","text":"This function is used to send task(s) to the HTC-Grid. Request Syntax gridConnector . send ( tasks_list = [ {}, ] ) Parameters tasks_list (list) [REQUIRED] A list of serializable dictionaries. Each dictionary will be passed to the execution lambda function as an event argument. Each dictionary will be encoded to base 64 before being stored in the Data Plane and then decoded before being passed to the execution lambda function. Output produced by the lambda function will be passed the same way in the reverse direction. Encoding and decoding is done by the grid connector, client only needs to provide serializable dictionary as input and output of the lambda functions. input : base64 . b64encode ( json . dumps ( input_dict ) . encode ( 'utf-8' )) output : base64 . b64decode ( output_dict ) . decode ( 'utf-8' ) Return type Dict Returns On successful submission, function returns a dictionary. { 'session_id' : 'string' , 'task_ids' : [ 'string' , ], } session_id - a single session ID that is associated with the submission. task_ids - an ordered list of task IDs associated with each task that was submitted in the request.","title":"Method - send"},{"location":"api/client/python/reference/#method-get_results","text":"Blocking function, waits until all tasks in the session are completed or until the timeout is expired. Function returns task IDs that have reached their terminal state (i.e., their states will not change). - Note , function does not return outputs of the lambdas, it is responsibility of the client to retrieve results from the Data Plane asynchronously. This function merely indicates that tasks are completed and results can be retrieved from the Data Plane. Request Syntax gridConnector . get_results ( submission_response = { 'session_id' : 'string' , 'task_ids' : [ 'string' , ], } timeout_sec = 'number' ) Parameters submission_response - a dictionary that was returned after successful submission of tasks. submission_response must contain a valid session_id and a list of associated task_ids . timeout_sec - the function will periodically (based on dynamodb_results_pull_interval_sec ) will try to pull for results until all tasks in the session are reached their terminal states or until the timeout is expires. The function uses the length of the task_ids list to determine the number of expected responses from the grid. Return type Dict Returns { 'finished' : [ 'string' , ] 'finished_OUTPUT' : [ 'string' , ], 'failed' : [ 'string' , ] 'failed_OUTPUT' : [ 'string' , ], 'cancelled' : [ 'string' , ] 'cancelled_OUTPUT' : [ 'string' , ], 'metadata' : { 'tasks_in_response' : 'number' } } * finished (optional) - list of finished task IDs * finished_OUTPUT (optional) - returns a string output produced by the lambda function * cancelled (optional) list of canceled task IDs * cancelled_OUTPUT (optional) - returns a hard-coded string read_from_REDIS indicating that actual output should be read from Data Plane, it is responsibility of the client to do that. * failed (optional) list of failed task IDs * failed_OUTPUT (optional) - returns a hard-coded string read_from_REDIS indicating that actual output should be read from Data Plane, it is responsibility of the client to do that. * metadata * tasks_in_response - total number of task in the terminal state (finished + failed + canceled) returned to the response. For example, if none of the tasks in the session have reached their terminal states an expected return is as follows: { 'metadata' : { 'tasks_in_response' : 0 } }","title":"Method - get_results"},{"location":"api/client/python/reference/#method-cancel_sessions","text":"Request Syntax response = gridConnector . cancel_sessions ( [ 'string' , ] ) Parameters Function takes list of session IDs to be canceled Return type Dict Returns Function returns dictionary of processed session IDs. { 'string' : { 'cancelled_retrying' : 0 , 'cancelled_pending' : 3 , 'cancelled_processing' : 1 , 'total_cancelled_tasks' : 4 }, 'string' : { .... }, #session - 2 'string' : { .... }, #session - 3 } string - keys of the response dictionary are session IDs that were passed in for cancellation. Sub-dictionaries contain information about how many tasks were moved into canceled state. cancelled_retying - number of tasks moved from retrying state into canceled state cancelled_pending - number of tasks moved from pending state into canceled state cancelled_processing - number of tasks moved from the processing state into canceled state. total_cancelled_tasks - total number of tasks that has been affected by this invocation.","title":"Method - cancel_sessions"},{"location":"api/client/python/usage/","text":"How to use API in your application \u00b6 This section outlines how to integrate the HTC grid with your current application. How to use the API in your application \u00b6 Below you will find a python example that demonstrate how to use the API in your application from api.connector import AWSConnector import os import json import logging client_config_file = os . environ [ 'AGENT_CONFIG_FILE' ] with open ( client_config_file , 'r' ) as file : client_config_file = json . loads ( file . read ()) if __name__ == \"__main__\" : logging . info ( \"Simple Client\" ) gridConnector = AWSConnector () gridConnector . init ( client_config_file , username = username , password = password ) gridConnector . authenticate () task_1_definition = { \"worker_arguments\" : [ \"1000\" , \"1\" , \"1\" ] } task_2_definition = { \"worker_arguments\" : [ \"2000\" , \"1\" , \"1\" ] } submission_resp = gridConnector . send ([ task_1_definition , task_2_definition ]) logging . info ( submission_resp ) results = gridConnector . get_results ( submission_resp , timeout_sec = 100 ) logging . info ( results ) Current release of HTC-Grid includes only Python3 API. However, if required, it is possible to develop a custom API using different languages (e.g., Java, .Net, etc.). Existing API is very concise and relies on the AWS API to interact with AWS services (refer to./source/client/python/api-v0.1/api for the example). How to run a client application \u00b6 Running a Client Application as a pod on EKS \u00b6 This is the easiest way to deploy a client application for testing purposes. Overview: 1. A client application is being packaged locally into a container. 2. The container is being deployed on the same EKS cluster governed by HTC-Grid as Job. 3. Once container is deployed, it launches the client application that submits the tasks. \u00b6 Details: 1. Build and Push docker image Build an image that will have all dependencies to be able to execute the client. See example in examples/submissions/k8s_jobs/Dockerfile.Submitter . In the example below we copy client.py into a container and install all dependencies form the requirements.txt. ```Docker FROM python:3.7.7-slim-buster RUN mkdir -p /app/py_connector RUN mkdir -p /dist COPY ./dist/* /dist/ COPY ./examples/client/python/requirements.txt /app/py_connector/ WORKDIR /app/py_connector RUN pip install -r requirements.txt COPY ./examples/client/python/client.py . ``` Push the image into the registry ```Makefile SUBMITTER_IMAGE_NAME = submitter TAG = <the tag specified during the HTC-Grid deployment> DOCKER_REGISTRY = $( ACCOUNT_ID ) .dkr.ecr. $( REGION ) .amazonaws.com docker push $(DOCKER_REGISTRY)/$(SUBMITTER_IMAGE_NAME) : $( TAG ) ``` Create a deployment .yaml file. For example: apiVersion : batch/v1 kind : Job metadata : name : single-task spec : template : spec : containers : - name : generator securityContext : {} image : XXXXXX.dkr.ecr.eu-west-1.amazonaws.com/submitter:XXXX imagePullPolicy : Always resources : limits : cpu : 100m memory : 128Mi requests : cpu : 100m memory : 128Mi command : [ \"python3\" , \"./simple_client.py\" , \"-n\" , \"1\" , \"--worker_arguments\" , \"1000 1 1\" , \"--job_size\" , \"1\" , \"--job_batch_size\" , \"1\" , \"--log\" , \"warning\" ] volumeMounts : - name : agent-config-volume mountPath : /etc/agent env : - name : INTRA_VPC value : \"1\" restartPolicy : Never nodeSelector : grid/type : Operator tolerations : - effect : NoSchedule key : grid/type operator : Equal value : Operator volumes : - name : agent-config-volume configMap : name : agent-configmap backoffLimit : 0 For the examples provided with HTC-Grid, these .yaml files are generated automatically when make happy-path command is executed. Each .yaml file is a template (e.g., examples/submissions/k8s_jobs/single-task-test.yaml.tpl) that is being filled in with relevant fields that match the AWS account and deployment configuration. The substitution is done by examples/submissions/k8s_jobs/Makefile and relevant attributes are passed in by the ./Makefile. Note, once docker image is built and pushed into registry, the yaml file contains the command parameter that tells how to launch the client application once container is running. The deployment yaml file can be generated by extending existing build process, written manually, or generated in any other way that suited the workload. To launch the client, simply execute the following command. kubectl apply -f ./generated/<custom.yaml.file>.yaml Running a Client Application locally \u00b6","title":"How to use API in your application"},{"location":"api/client/python/usage/#how-to-use-api-in-your-application","text":"This section outlines how to integrate the HTC grid with your current application.","title":"How to use API in your application"},{"location":"api/client/python/usage/#how-to-use-the-api-in-your-application","text":"Below you will find a python example that demonstrate how to use the API in your application from api.connector import AWSConnector import os import json import logging client_config_file = os . environ [ 'AGENT_CONFIG_FILE' ] with open ( client_config_file , 'r' ) as file : client_config_file = json . loads ( file . read ()) if __name__ == \"__main__\" : logging . info ( \"Simple Client\" ) gridConnector = AWSConnector () gridConnector . init ( client_config_file , username = username , password = password ) gridConnector . authenticate () task_1_definition = { \"worker_arguments\" : [ \"1000\" , \"1\" , \"1\" ] } task_2_definition = { \"worker_arguments\" : [ \"2000\" , \"1\" , \"1\" ] } submission_resp = gridConnector . send ([ task_1_definition , task_2_definition ]) logging . info ( submission_resp ) results = gridConnector . get_results ( submission_resp , timeout_sec = 100 ) logging . info ( results ) Current release of HTC-Grid includes only Python3 API. However, if required, it is possible to develop a custom API using different languages (e.g., Java, .Net, etc.). Existing API is very concise and relies on the AWS API to interact with AWS services (refer to./source/client/python/api-v0.1/api for the example).","title":"How to use the API in your application"},{"location":"api/client/python/usage/#how-to-run-a-client-application","text":"","title":"How to run a client application"},{"location":"api/client/python/usage/#running-a-client-application-as-a-pod-on-eks","text":"This is the easiest way to deploy a client application for testing purposes. Overview: 1. A client application is being packaged locally into a container. 2. The container is being deployed on the same EKS cluster governed by HTC-Grid as Job. 3. Once container is deployed, it launches the client application that submits the tasks.","title":"Running a Client Application as a pod on EKS"},{"location":"api/client/python/usage/#_1","text":"Details: 1. Build and Push docker image Build an image that will have all dependencies to be able to execute the client. See example in examples/submissions/k8s_jobs/Dockerfile.Submitter . In the example below we copy client.py into a container and install all dependencies form the requirements.txt. ```Docker FROM python:3.7.7-slim-buster RUN mkdir -p /app/py_connector RUN mkdir -p /dist COPY ./dist/* /dist/ COPY ./examples/client/python/requirements.txt /app/py_connector/ WORKDIR /app/py_connector RUN pip install -r requirements.txt COPY ./examples/client/python/client.py . ``` Push the image into the registry ```Makefile SUBMITTER_IMAGE_NAME = submitter TAG = <the tag specified during the HTC-Grid deployment> DOCKER_REGISTRY = $( ACCOUNT_ID ) .dkr.ecr. $( REGION ) .amazonaws.com docker push $(DOCKER_REGISTRY)/$(SUBMITTER_IMAGE_NAME) : $( TAG ) ``` Create a deployment .yaml file. For example: apiVersion : batch/v1 kind : Job metadata : name : single-task spec : template : spec : containers : - name : generator securityContext : {} image : XXXXXX.dkr.ecr.eu-west-1.amazonaws.com/submitter:XXXX imagePullPolicy : Always resources : limits : cpu : 100m memory : 128Mi requests : cpu : 100m memory : 128Mi command : [ \"python3\" , \"./simple_client.py\" , \"-n\" , \"1\" , \"--worker_arguments\" , \"1000 1 1\" , \"--job_size\" , \"1\" , \"--job_batch_size\" , \"1\" , \"--log\" , \"warning\" ] volumeMounts : - name : agent-config-volume mountPath : /etc/agent env : - name : INTRA_VPC value : \"1\" restartPolicy : Never nodeSelector : grid/type : Operator tolerations : - effect : NoSchedule key : grid/type operator : Equal value : Operator volumes : - name : agent-config-volume configMap : name : agent-configmap backoffLimit : 0 For the examples provided with HTC-Grid, these .yaml files are generated automatically when make happy-path command is executed. Each .yaml file is a template (e.g., examples/submissions/k8s_jobs/single-task-test.yaml.tpl) that is being filled in with relevant fields that match the AWS account and deployment configuration. The substitution is done by examples/submissions/k8s_jobs/Makefile and relevant attributes are passed in by the ./Makefile. Note, once docker image is built and pushed into registry, the yaml file contains the command parameter that tells how to launch the client application once container is running. The deployment yaml file can be generated by extending existing build process, written manually, or generated in any other way that suited the workload. To launch the client, simply execute the following command. kubectl apply -f ./generated/<custom.yaml.file>.yaml","title":""},{"location":"api/client/python/usage/#running-a-client-application-locally","text":"","title":"Running a Client Application locally"},{"location":"api/worker/","text":"Developing a Worker Function \u00b6 HTC-Grid uses Amazon Elastic Kubernetes Service (Amazon EKS) as a computational back-end. Each engine is a pod running two containers: (i) an Agent and a (ii) Worker Function. The Agent provides a connectivity layer between the HTC-Grid and the Worker container. Agent's responsibilities include: (i) pulling for new tasks, (ii) interacting with the Data Plane (I/O), (iii) sending heartbeats back to Control Plane, and (iv) indicating completion of a task. Note, Agent does not need to be changed when developing new applications on HTC-Grid. The Worker container executes the custom code that performs the computational task. The execution is done locally within the container. The code of the worker function needs to be modified during the development. Note: depending on the workload it is possible for the Worker function to access HTC-Grid's Data Plane directly or to access any other external systems as might be required. This functionality is not provided as part of the HTC-Grid. At the high level the development process involves 4 steps: Write a custom Worker code for the target workload. Package all the dependencies into a docker container which also includes custom Lambda runtime that will be used to execute worker function. Use this container to compile & test your code Zip the compiled function along with any dependencies and upload to an S3 bucket (default bucket name is stored in $S3_LAMBDA_HTCGRID_BUCKET_NAME) (if S3 bucket is different from $S3_LAMBDA_HTCGRID_BUCKET_NAME) Update HTC-Grid configuration to point to the new location of the target Zip file.","title":"Developing a Worker Function"},{"location":"api/worker/#developing-a-worker-function","text":"HTC-Grid uses Amazon Elastic Kubernetes Service (Amazon EKS) as a computational back-end. Each engine is a pod running two containers: (i) an Agent and a (ii) Worker Function. The Agent provides a connectivity layer between the HTC-Grid and the Worker container. Agent's responsibilities include: (i) pulling for new tasks, (ii) interacting with the Data Plane (I/O), (iii) sending heartbeats back to Control Plane, and (iv) indicating completion of a task. Note, Agent does not need to be changed when developing new applications on HTC-Grid. The Worker container executes the custom code that performs the computational task. The execution is done locally within the container. The code of the worker function needs to be modified during the development. Note: depending on the workload it is possible for the Worker function to access HTC-Grid's Data Plane directly or to access any other external systems as might be required. This functionality is not provided as part of the HTC-Grid. At the high level the development process involves 4 steps: Write a custom Worker code for the target workload. Package all the dependencies into a docker container which also includes custom Lambda runtime that will be used to execute worker function. Use this container to compile & test your code Zip the compiled function along with any dependencies and upload to an S3 bucket (default bucket name is stored in $S3_LAMBDA_HTCGRID_BUCKET_NAME) (if S3 bucket is different from $S3_LAMBDA_HTCGRID_BUCKET_NAME) Update HTC-Grid configuration to point to the new location of the target Zip file.","title":"Developing a Worker Function"},{"location":"api/worker/c%2B%2B/development/","text":"Developing C++ Worker Function \u00b6 Writing a C++ Worker Function requires creation of an additional shell script: bootstrap . The bootstrap script is a simple wrapper that takes inputs from Agent and passes it to the C++ executable, similarly it takes the response and passes it back to the Agent once task is done. See example below: An example of a complete bootstrap can be found here: examples/workloads/c++/mock_computation/bootstrap , although, a custom version of the bootstrap script will be required for the custom Worker function. The bootstrap script takes task's definition as a string and passes them to the executable as an argument. C++ executable does not need to have a lambda_handler method implemented, instead, the execution starts at the main method. C++ Including Dependencies \u00b6 Packaging all dependencies and uploading them to an S3 bucket is generally the same as for the Python3 runtime. However, C++ requires an additional step of compiling the source code before zipping all the dependencies. Compiling all the dependencies in the container guarantees that the executable will run in the provided runtime once deployed in HTC-Grid. Refer to a complete example here: examples/workloads/c++/mock_computation/Dockerfile.Build # Snippet example: ... COPY mock_compute_engine.cpp . COPY Makefile . #Compile the executable in the runtime environment. RUN make main ... 3. Configuring HTC-Grid Deployment \u00b6 There are no additional changes required to HTC-Grid to define/launch new Client applications. Some changes might be required to update Worker functions, see below The root ./Makefile has 3 options for building and uploading sample Worker functions (e.g., upload-c++ , upload-python , and upload-python-ql ). These options simply automate all steps described in Section 2 \"Developing a Worker Function\". Follow these examples to build & upload custom worker function code. To execute each option in isolation, specify the option with the make (i.e., instead of happy-path). make upload-c++ TAG = $TAG ACCOUNT_ID = $HTCGRID_ACCOUNT_ID REGION = $HTCGRID_REGION BUCKET_NAME = $S3_LAMBDA_HTCGRID_BUCKET_NAME The above steps will upload new Worker Function zip in S3 bucket. However, only new worker pods will be able to benefit from this update. To apply the changes to the entire deployment it is necessary to remove all currently running worker pods (e.g., by executing kubectl delete $(kubectl get po -o name) ). Each compute environment pod starts by executing lambda-init container (defined at source/compute_plane/shell/attach-layer ) which pulls the Worker function zip package from the S3_LAMBDA_HTCGRID_BUCKET_NAME S3 bucket at the pod boot time.","title":"Developing C++ Worker Function"},{"location":"api/worker/c%2B%2B/development/#developing-c-worker-function","text":"Writing a C++ Worker Function requires creation of an additional shell script: bootstrap . The bootstrap script is a simple wrapper that takes inputs from Agent and passes it to the C++ executable, similarly it takes the response and passes it back to the Agent once task is done. See example below: An example of a complete bootstrap can be found here: examples/workloads/c++/mock_computation/bootstrap , although, a custom version of the bootstrap script will be required for the custom Worker function. The bootstrap script takes task's definition as a string and passes them to the executable as an argument. C++ executable does not need to have a lambda_handler method implemented, instead, the execution starts at the main method.","title":"Developing C++ Worker Function"},{"location":"api/worker/c%2B%2B/development/#c-including-dependencies","text":"Packaging all dependencies and uploading them to an S3 bucket is generally the same as for the Python3 runtime. However, C++ requires an additional step of compiling the source code before zipping all the dependencies. Compiling all the dependencies in the container guarantees that the executable will run in the provided runtime once deployed in HTC-Grid. Refer to a complete example here: examples/workloads/c++/mock_computation/Dockerfile.Build # Snippet example: ... COPY mock_compute_engine.cpp . COPY Makefile . #Compile the executable in the runtime environment. RUN make main ...","title":"C++ Including Dependencies"},{"location":"api/worker/c%2B%2B/development/#3-configuring-htc-grid-deployment","text":"There are no additional changes required to HTC-Grid to define/launch new Client applications. Some changes might be required to update Worker functions, see below The root ./Makefile has 3 options for building and uploading sample Worker functions (e.g., upload-c++ , upload-python , and upload-python-ql ). These options simply automate all steps described in Section 2 \"Developing a Worker Function\". Follow these examples to build & upload custom worker function code. To execute each option in isolation, specify the option with the make (i.e., instead of happy-path). make upload-c++ TAG = $TAG ACCOUNT_ID = $HTCGRID_ACCOUNT_ID REGION = $HTCGRID_REGION BUCKET_NAME = $S3_LAMBDA_HTCGRID_BUCKET_NAME The above steps will upload new Worker Function zip in S3 bucket. However, only new worker pods will be able to benefit from this update. To apply the changes to the entire deployment it is necessary to remove all currently running worker pods (e.g., by executing kubectl delete $(kubectl get po -o name) ). Each compute environment pod starts by executing lambda-init container (defined at source/compute_plane/shell/attach-layer ) which pulls the Worker function zip package from the S3_LAMBDA_HTCGRID_BUCKET_NAME S3 bucket at the pod boot time.","title":"3. Configuring HTC-Grid Deployment"},{"location":"api/worker/python/development/","text":"Developing Python3 Worker Function \u00b6 HTC-Grid comes with examples demonstrating how to build Python3 and C++ based Worker functions. The diagram below outlines the relationship between the Agent and the Worker function following Python3 example. HTC-Grid's Worker function follows the same API as AWS Lambda function. Following examples supplied with HTC-Grid ( examples/workloads/python/mock_computation/mock_compute_engine.py and examples/workloads/python/quant_lib/portfolio_pricing_engine.py ) and AWS Lambda documentation write a python module that implements a lambda_handler entry point. This is the function that will be invoked when Agent passes the task to the Worker function. Note, the entry python file and the handler function name can be re-defined in generated/python_runtime_grid_config.json (see below). To apply these changes, terraform apply needs to be re-executed under deployment/grid/terraform \"agent_configuration\" : { \"lambda\" : { ... \"lambda_handler_file_name\" : \"portfolio_pricing_engine\" , \"lambda_handler_function_name\" : \"lambda_handler\" } } The lambda_handler has two arguments an event and a context . - event - is the task's definition that was provided by the client application at the time of the task submission. - context -[To be defined] The return of the lambda_handler function will be treated as the result of the task's execution and will be stored in the Data Plane by the Agent. Subsequently, the Client application will be able to retrieve this output from the Data Plane. Python3 Including Dependencies \u00b6 HTC-Grid uses custom Lambda runtime lambda-rie which closely mimics the runtime of AWS Lambda. Custom Lambda runtime (along with any additional dependencies) should be included into the docker image to execute the Worker function. See example below: # 1. Include custom lambda runtime FROM public.ecr.aws/lambda/python:3.8 RUN yum install -d1 -y zip # 2. Create a directory which will hold all relevant dependencies e.g., [/app] RUN mkdir -p /app WORKDIR /app # 3. Copy all relevant local dependencies COPY portfolio_pricing_engine.py . COPY american_options.py . COPY european_options.py . COPY ql_common.py . # 4. Install any third party dependencies into the [/app] folder RUN pip install --target = /app QuantLib # 5. Zip the content of the [/app] which holds all the dependencies at this stage RUN mkdir -p /app/build RUN zip -yr lambda.zip . ENTRYPOINT cp lambda.zip /app/build The content of the produced zip file should be copied into an S3 bucket. The name of the bucket is stored in S3_LAMBDA_HTCGRID_BUCKET_NAME environmental variable which is set during the deployment stage of the HTC-Grid allowing the grid to retrieve and deploy the correct Worker function package at runtime. For the complete examples please refer to one of the Makefiles in examples/workloads directory.","title":"Developing Python3 Worker Function"},{"location":"api/worker/python/development/#developing-python3-worker-function","text":"HTC-Grid comes with examples demonstrating how to build Python3 and C++ based Worker functions. The diagram below outlines the relationship between the Agent and the Worker function following Python3 example. HTC-Grid's Worker function follows the same API as AWS Lambda function. Following examples supplied with HTC-Grid ( examples/workloads/python/mock_computation/mock_compute_engine.py and examples/workloads/python/quant_lib/portfolio_pricing_engine.py ) and AWS Lambda documentation write a python module that implements a lambda_handler entry point. This is the function that will be invoked when Agent passes the task to the Worker function. Note, the entry python file and the handler function name can be re-defined in generated/python_runtime_grid_config.json (see below). To apply these changes, terraform apply needs to be re-executed under deployment/grid/terraform \"agent_configuration\" : { \"lambda\" : { ... \"lambda_handler_file_name\" : \"portfolio_pricing_engine\" , \"lambda_handler_function_name\" : \"lambda_handler\" } } The lambda_handler has two arguments an event and a context . - event - is the task's definition that was provided by the client application at the time of the task submission. - context -[To be defined] The return of the lambda_handler function will be treated as the result of the task's execution and will be stored in the Data Plane by the Agent. Subsequently, the Client application will be able to retrieve this output from the Data Plane.","title":"Developing Python3 Worker Function"},{"location":"api/worker/python/development/#python3-including-dependencies","text":"HTC-Grid uses custom Lambda runtime lambda-rie which closely mimics the runtime of AWS Lambda. Custom Lambda runtime (along with any additional dependencies) should be included into the docker image to execute the Worker function. See example below: # 1. Include custom lambda runtime FROM public.ecr.aws/lambda/python:3.8 RUN yum install -d1 -y zip # 2. Create a directory which will hold all relevant dependencies e.g., [/app] RUN mkdir -p /app WORKDIR /app # 3. Copy all relevant local dependencies COPY portfolio_pricing_engine.py . COPY american_options.py . COPY european_options.py . COPY ql_common.py . # 4. Install any third party dependencies into the [/app] folder RUN pip install --target = /app QuantLib # 5. Zip the content of the [/app] which holds all the dependencies at this stage RUN mkdir -p /app/build RUN zip -yr lambda.zip . ENTRYPOINT cp lambda.zip /app/build The content of the produced zip file should be copied into an S3 bucket. The name of the bucket is stored in S3_LAMBDA_HTCGRID_BUCKET_NAME environmental variable which is set during the deployment stage of the HTC-Grid allowing the grid to retrieve and deploy the correct Worker function package at runtime. For the complete examples please refer to one of the Makefiles in examples/workloads directory.","title":"Python3 Including Dependencies"},{"location":"architecture/architecture/","text":"HTC-Grid Architecture \u00b6 This document outlines high level architecture and API of HTC-Grid. Definitions \u00b6 Client Application - A software system that generates job requests and retrieves computation results from the grid system. Task - a unit of work to be scheduled for an execution. A task may have an associated task input and produce an output. The interface of a task takes the same form as the interface of an AWS Lambda handler ( Python , Go , Java , C# , etc). Session - a vector of tasks. For example, a job may define a series of scenarios and how they are sub-divided into a set of tasks. Such job can be submitted as a single session containing multiple tasks. Task Input - the set of data which is required in addition to the job definition. Task Input is passed to Engines by reference, bypassing the scheduler itself. The Engine - is a software component responsible for invoking the task execution. High Level Architecture \u00b6 This section outlines the high level modular architecture of the cloud native HTC-Grid HTC-Grid has been designed with strong focus on the following tenets: use of cloud native serverless and fully managed services, performance & scalability, availability, cost optimization, and operational simplicity. The grid system is composed of 4 functional components: 1. HTC-Grid\u2019s API provides entry point for Client Applications to interact with the grid, 2. Data Plane facilitates storage, and I/O operations for submitting jobs\u2019 definitions and retrieving computational results, 3. Control Plane (i.e., scheduler) keeps track of the task's execution, grid\u2019s scaling, and error handling 4. A pool of Computing Resources that perform computational tasks. Each component has a clearly defined role and operates strictly within that role. Inter module communication is implemented using standardized AWS API which facilitates independent development and provide further customization options. Internally, each of the 4 functional components (API, Data & Control Planes, and Compute Resources) are built using exclusively cloud native building blocks such as: serverless functions and fully managed services. These blocks require no human maintenance (zero administration), are highly available by design, and can scale horizontally in response to demand. API: Interacting with HTC-Grid \u00b6 Figure below demonstrates high level steps involved in the task submission and result retrieval. HTC-Grid allows client applications to submit a session (job) containing a single task, or a vector of tasks. Each submission generates a system-wide unique session_id which is associated with the submission, the session_id is returned to the client application. Successful reception of a session_id indicates that all the tasks of the job are in the system and eventually will be executed. Client applications can use session_id to inquire the state of the tasks within the session (e.g., pending, running, failed, completed, etc.) and subsequently retrieve results once all the tasks of the session are completed. A session is considered to be completed once all tasks of the session are completed. Additionally, if the session did not complete within specified timeout, the session is considered to be failed. During a normal usage, client application would either (i) save the returned session object locally and submit more sessions (jobs) (in case of a batch of jobs) or will wait for the completion of the last submitted session. Note, each session can have a list of tasks associated with it. A multi session submission is also possible, in that case a list of session IDs is returned by the connection object. Control Plane \u00b6 Control Plane performs the role of a job broker responsible for coordinating and scheduling jobs executions in the grid along with scaling Compute Resources in accordance with demand. Control Plane has built in failure detection and recovery mechanism which allows it to retry and report failed jobs. All building components of the Control Plane are fully managed AWS services (DynamoDB, Simple Queue Service, API Gateway) or serverless functions (i.e., Lambda) which minimizes management and simplifies design. Failure Detection and Recover \u00b6 Engines acquire tasks by pulling SQS queues, respecting the rank of priority. Once a task has been received by an Engine, the Engine performs an atomic write transaction in DynamoDB to change the status of the task from \u201cpending\u201d to \u201cprocessing\u201d. At this stage a task is associated with that Engine. Failure detection in HTC-Grid is implemented via heart beat mechanism. While the task is being processed, the Engine periodically emits heart-beat messages that update the row corresponding to the task in DynamoDB. These periodic updates indicate to the Control Plane that the Engine is alive and still processing the task. Failure recovery and state reconciliation is implemented using a scheduled Lambda function. This lambda function regularly queries DynamoDB for tasks that are in the processing state but did not receive heart beats from the associated Engines for too long. This indicates that the associated Engines have failed. Depending on the job definition, failed tasks can be retried up to a fixed number of times (by being re-inserted into the queue) or permanently moved into a \u2018failed\u2019 SQS queue for later analysis, following a dead letter queue ( https://en.wikipedia.org/wiki/Dead_letter_queue ) pattern. All failure events are reported. When the task is completed, the Engine updates DynamoDB for the last time and sets task status to \u201ccompleted\u201d. Afterwards, the Engine tries to acquire a next task from an SQS queue. Data Plane \u00b6 The Data plane is responsible for data distribution across the grid system. Specifically it serves two purposes (i) stores tasks input data associated with jobs definitions (client-to-grid) and (ii) stores results of the computation (grid-to-client). HTC Grid can use S3 or Redis as back-end for the data plane depending on the requirements. Alternatively, existing interface can easily be extended to support other storage systems. Compute Resources \u00b6 HTC-Grid utilizes Amazon Elastic Kubernetes Service (Amazon EKS) as a computational back-end. Each engine is a pod containing two containers an Agent and a Lambda. The Lambda container executes lambda locally within the container (there are no calls made to AWS lambda service, the execution is done within the node Lambda container). The agent provides a connectivity layer between the HTC-Grid and the Lambda container. The Agent pulls new tasks from the task queues in the Control Plane, once a new task is acquired the agent invokes the Lambda container and passes the task definition. The Lambda container contains custom executable that perform the work. It is responsibility of the Lambda container to connect to the Data Plane and retrieve associated task payload. Once the task is complete, the results is uploaded to the Data Plane. The Grid Agent updates the task\u2019s state to \u201ccompleted\u201d and pulls the next task from the Control Plane. Other Functions \u00b6 Clients can be called from Step functions to automate complex application dependencies. Multiple instances of the HTC-Grid can be deployed across multiple on the same account and same region or in multiple regions with the client application running either on AWS or on the cloud.","title":"HTC-Grid Architecture"},{"location":"architecture/architecture/#htc-grid-architecture","text":"This document outlines high level architecture and API of HTC-Grid.","title":"HTC-Grid Architecture"},{"location":"architecture/architecture/#definitions","text":"Client Application - A software system that generates job requests and retrieves computation results from the grid system. Task - a unit of work to be scheduled for an execution. A task may have an associated task input and produce an output. The interface of a task takes the same form as the interface of an AWS Lambda handler ( Python , Go , Java , C# , etc). Session - a vector of tasks. For example, a job may define a series of scenarios and how they are sub-divided into a set of tasks. Such job can be submitted as a single session containing multiple tasks. Task Input - the set of data which is required in addition to the job definition. Task Input is passed to Engines by reference, bypassing the scheduler itself. The Engine - is a software component responsible for invoking the task execution.","title":"Definitions"},{"location":"architecture/architecture/#high-level-architecture","text":"This section outlines the high level modular architecture of the cloud native HTC-Grid HTC-Grid has been designed with strong focus on the following tenets: use of cloud native serverless and fully managed services, performance & scalability, availability, cost optimization, and operational simplicity. The grid system is composed of 4 functional components: 1. HTC-Grid\u2019s API provides entry point for Client Applications to interact with the grid, 2. Data Plane facilitates storage, and I/O operations for submitting jobs\u2019 definitions and retrieving computational results, 3. Control Plane (i.e., scheduler) keeps track of the task's execution, grid\u2019s scaling, and error handling 4. A pool of Computing Resources that perform computational tasks. Each component has a clearly defined role and operates strictly within that role. Inter module communication is implemented using standardized AWS API which facilitates independent development and provide further customization options. Internally, each of the 4 functional components (API, Data & Control Planes, and Compute Resources) are built using exclusively cloud native building blocks such as: serverless functions and fully managed services. These blocks require no human maintenance (zero administration), are highly available by design, and can scale horizontally in response to demand.","title":"High Level Architecture"},{"location":"architecture/architecture/#api-interacting-with-htc-grid","text":"Figure below demonstrates high level steps involved in the task submission and result retrieval. HTC-Grid allows client applications to submit a session (job) containing a single task, or a vector of tasks. Each submission generates a system-wide unique session_id which is associated with the submission, the session_id is returned to the client application. Successful reception of a session_id indicates that all the tasks of the job are in the system and eventually will be executed. Client applications can use session_id to inquire the state of the tasks within the session (e.g., pending, running, failed, completed, etc.) and subsequently retrieve results once all the tasks of the session are completed. A session is considered to be completed once all tasks of the session are completed. Additionally, if the session did not complete within specified timeout, the session is considered to be failed. During a normal usage, client application would either (i) save the returned session object locally and submit more sessions (jobs) (in case of a batch of jobs) or will wait for the completion of the last submitted session. Note, each session can have a list of tasks associated with it. A multi session submission is also possible, in that case a list of session IDs is returned by the connection object.","title":"API: Interacting with HTC-Grid"},{"location":"architecture/architecture/#control-plane","text":"Control Plane performs the role of a job broker responsible for coordinating and scheduling jobs executions in the grid along with scaling Compute Resources in accordance with demand. Control Plane has built in failure detection and recovery mechanism which allows it to retry and report failed jobs. All building components of the Control Plane are fully managed AWS services (DynamoDB, Simple Queue Service, API Gateway) or serverless functions (i.e., Lambda) which minimizes management and simplifies design.","title":"Control Plane"},{"location":"architecture/architecture/#failure-detection-and-recover","text":"Engines acquire tasks by pulling SQS queues, respecting the rank of priority. Once a task has been received by an Engine, the Engine performs an atomic write transaction in DynamoDB to change the status of the task from \u201cpending\u201d to \u201cprocessing\u201d. At this stage a task is associated with that Engine. Failure detection in HTC-Grid is implemented via heart beat mechanism. While the task is being processed, the Engine periodically emits heart-beat messages that update the row corresponding to the task in DynamoDB. These periodic updates indicate to the Control Plane that the Engine is alive and still processing the task. Failure recovery and state reconciliation is implemented using a scheduled Lambda function. This lambda function regularly queries DynamoDB for tasks that are in the processing state but did not receive heart beats from the associated Engines for too long. This indicates that the associated Engines have failed. Depending on the job definition, failed tasks can be retried up to a fixed number of times (by being re-inserted into the queue) or permanently moved into a \u2018failed\u2019 SQS queue for later analysis, following a dead letter queue ( https://en.wikipedia.org/wiki/Dead_letter_queue ) pattern. All failure events are reported. When the task is completed, the Engine updates DynamoDB for the last time and sets task status to \u201ccompleted\u201d. Afterwards, the Engine tries to acquire a next task from an SQS queue.","title":"Failure Detection and Recover"},{"location":"architecture/architecture/#data-plane","text":"The Data plane is responsible for data distribution across the grid system. Specifically it serves two purposes (i) stores tasks input data associated with jobs definitions (client-to-grid) and (ii) stores results of the computation (grid-to-client). HTC Grid can use S3 or Redis as back-end for the data plane depending on the requirements. Alternatively, existing interface can easily be extended to support other storage systems.","title":"Data Plane"},{"location":"architecture/architecture/#compute-resources","text":"HTC-Grid utilizes Amazon Elastic Kubernetes Service (Amazon EKS) as a computational back-end. Each engine is a pod containing two containers an Agent and a Lambda. The Lambda container executes lambda locally within the container (there are no calls made to AWS lambda service, the execution is done within the node Lambda container). The agent provides a connectivity layer between the HTC-Grid and the Lambda container. The Agent pulls new tasks from the task queues in the Control Plane, once a new task is acquired the agent invokes the Lambda container and passes the task definition. The Lambda container contains custom executable that perform the work. It is responsibility of the Lambda container to connect to the Data Plane and retrieve associated task payload. Once the task is complete, the results is uploaded to the Data Plane. The Grid Agent updates the task\u2019s state to \u201ccompleted\u201d and pulls the next task from the Control Plane.","title":"Compute Resources"},{"location":"architecture/architecture/#other-functions","text":"Clients can be called from Step functions to automate complex application dependencies. Multiple instances of the HTC-Grid can be deployed across multiple on the same account and same region or in multiple regions with the client application running either on AWS or on the cloud.","title":"Other Functions"},{"location":"getting_started/","text":"Introduction \u00b6 This section steps through the HTC-Grid's AWS infrastructure and software prerequisites. An AWS account is required along with some limited familiarity of AWS services and terraform. The execution of the Happy Path section will create AWS resources not included in the free tier and then will incur cost to your AWS Account. The complete execution of this section will cost at least 50$ per day.","title":"Introduction"},{"location":"getting_started/#introduction","text":"This section steps through the HTC-Grid's AWS infrastructure and software prerequisites. An AWS account is required along with some limited familiarity of AWS services and terraform. The execution of the Happy Path section will create AWS resources not included in the free tier and then will incur cost to your AWS Account. The complete execution of this section will cost at least 50$ per day.","title":"Introduction"},{"location":"getting_started/configurations/","text":"Configuring Local Environment \u00b6 AWS CLI \u00b6 Configure the AWS CLI to use your AWS account: see https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html Check connectivity as follows: $ aws sts get-caller-identity { \"Account\" : \"XXXXXXXXXXXX\" , \"UserId\" : \"XXXXXXXXXXXXXXXXXXXXX\" , \"Arn\" : \"arn:aws:iam::XXXXXXXXXXXX:user/XXXXXXX\" } Python \u00b6 The current release of HTC requires python3.7, and the documentation assumes the use of virtualenv . Set this up as follows: $ cd <project_root>/ $ virtualenv --python = $PATH /python3.7 venv created virtual environment CPython3.7.10.final.0-64 in 1329ms creator CPython3Posix ( dest = <project_roor>/venv, clear = False, no_vcs_ignore = False, global = False ) seeder FromAppData ( download = False, pip = bundle, setuptools = bundle, wheel = bundle, via = copy, app_data_dir = /Users/user/Library/Application Support/virtualenv ) added seed packages: pip == 21 .0.1, setuptools == 54 .1.2, wheel == 0 .36.2 activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator Check you have the correct version of python ( 3.7.x ), with a path rooted on <project_root> , then start the environment: $ source ./venv/bin/activate (venv) 8c8590cffb8f:htc-grid-0.0.1 $ Check the python version as follows: $ which python <project_root>/venv/bin/python $ python -V Python 3 .7.10 For further details on virtualenv see https://sourabhbajaj.com/mac-setup/Python/virtualenv.html","title":"Configuring Local Environment"},{"location":"getting_started/configurations/#configuring-local-environment","text":"","title":"Configuring Local Environment"},{"location":"getting_started/configurations/#aws-cli","text":"Configure the AWS CLI to use your AWS account: see https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html Check connectivity as follows: $ aws sts get-caller-identity { \"Account\" : \"XXXXXXXXXXXX\" , \"UserId\" : \"XXXXXXXXXXXXXXXXXXXXX\" , \"Arn\" : \"arn:aws:iam::XXXXXXXXXXXX:user/XXXXXXX\" }","title":"AWS CLI"},{"location":"getting_started/configurations/#python","text":"The current release of HTC requires python3.7, and the documentation assumes the use of virtualenv . Set this up as follows: $ cd <project_root>/ $ virtualenv --python = $PATH /python3.7 venv created virtual environment CPython3.7.10.final.0-64 in 1329ms creator CPython3Posix ( dest = <project_roor>/venv, clear = False, no_vcs_ignore = False, global = False ) seeder FromAppData ( download = False, pip = bundle, setuptools = bundle, wheel = bundle, via = copy, app_data_dir = /Users/user/Library/Application Support/virtualenv ) added seed packages: pip == 21 .0.1, setuptools == 54 .1.2, wheel == 0 .36.2 activators BashActivator,CShellActivator,FishActivator,PowerShellActivator,PythonActivator,XonshActivator Check you have the correct version of python ( 3.7.x ), with a path rooted on <project_root> , then start the environment: $ source ./venv/bin/activate (venv) 8c8590cffb8f:htc-grid-0.0.1 $ Check the python version as follows: $ which python <project_root>/venv/bin/python $ python -V Python 3 .7.10 For further details on virtualenv see https://sourabhbajaj.com/mac-setup/Python/virtualenv.html","title":"Python"},{"location":"getting_started/happy-path/","text":"Happy Path \u00b6 Installing the HTC-Grid software \u00b6 Unpack the provided HTC-Grid software ZIP (i.e: htc-grid-0.3.0.tar.gz ) or clone the repository into a local directory of your choice; this directory referred in this documentation as <project_root> . Unless stated otherwise, all paths referenced in this documentation are relative to <project_root> . For first time users or Windows users, we do recommend the use of Cloud9 as the platform to deploy HTC-Grid. The installation process uses Terraform and also make to build up artifacts and environment. This project provides a CloudFormation Cloud9 Stack that installs all the prerequisites-requisites listed above to deploy and develop HTC-Grid. Just follow the standard process in your account and deploy the Cloud9 CloudFormation Stack . Once the CloudFormation Stack has been created, open either the Output section in CloudFormation or go to Cloud9 in your AWS console and open the newly created Cloud9 environment. Define variables for deploying the infrastructure \u00b6 To simplify this installation it is suggested that a unique name (to be used later) is also used to prefix the different required bucket. TAG needs to follow S3 naming rules . export TAG = <Your tag> Define the region where the grid will be deployed export HTCGRID_REGION = <Your region> <Your region> region can be (the list is not exhaustive) eu-west-1 eu-west-2 eu-west-3 eu-central-1 us-east-1 us-west-2 ap-northeast-1 ap-southeast-1 Create the infrastructure for storing the state of the HTC Grid \u00b6 The following step creates 3 S3 buckets that will be needed during the installation: * 2 buckets will store the state of the different Terraform deployments * 1 bucket will store the HTC artifacts (the lambda to be executed by the agent) make init-grid-state TAG=$TAG REGION=$HTCGRID_REGION To validate the creation of the S3 buckets, you can run aws cloudformation describe-stacks --stack-name $TAG --region $HTCGRID_REGION --query 'Stacks[0]' That will list the 3 S3 Buckets that we just created. Create and deploy HTC-Grid images \u00b6 The HTC-Grid project has external software dependencies that are deployed as container images. Instead of downloading each time from the public DockerHub repository, this step will pull those dependencies and upload into the your Amazon Elastic Container Registry (ECR) . Important Note HTC-Grid uses a few open source project with container images stored at DockerHub . DockerHub has a download rate limit policy . This may impact you when running this step as an anonymous user as you can get errors when running the Terraform command below. To overcome those errors, you can re-run the Terraform command and wait until the throttling limit is lifted, or optionally you can create an account in hub.docker.com and then use the credentials of the account using docker login locally to avoid anonymous throttling limitations. As you'll be uploading images to ECR, to avoid timeouts, refresh your ECR authentication token: make ecr-login The following command will go to the ~/environment/aws-htc-grid/deployment/image_repository/terraform and initialize the Terraform project using the bucket $S3_IMAGE_TFSTATE_HTCGRID_BUCKET_NAME as the bucket that will hold the Terraform state: make init-images TAG=$TAG REGION=$HTCGRID_REGION If successful, you can now run terraform apply to create the HTC-Grid infrastructure. This can take between 10 and 15 minutes depending on the Internet connection. make transfer-images TAG = $TAG REGION = $HTCGRID_REGION The following command will list the repositories You can check which repositories have been created in the ECR console or by executing the command : aws ecr describe-repositories --query \"repositories[*].repositoryUri\" NB: This operation fetches images from external repositories and creates a copy into your ECR account, sometimes the fetch to external repositories may have temporary failures due to the state of the external repositories, If the terraform apply fails with errors such as the ones below, re-run the command until terraform apply successfully completes. name unknown: The repository with name 'xxxxxxxxx' does not exist in the registry with id Build HTC artifacts \u00b6 HTC artifacts include: python packages, docker images, configuration files for HTC and k8s. To build and install these: Now build the images for the HTC agent. Return to <project_root> and run the command: make happy-path TAG = $TAG REGION = $HTCGRID_REGION * If `TAG` is omitted then `mainline` will be the chosen has a default value . * If `REGION` is omitted then `eu-west-1` will be used . A folder name generated will be created at <project_root> . This folder should contain the following two files: * grid_config.json a configuration file for the grid with basic setting * single-task-test.yaml the Kubernetes configuration for running a single tasks on the grid. Configuring the HTC-Grid runtime \u00b6 The grid_config.json is ready to deploy, but you can tune it before deployment. Some important parameters are: * region : the AWS region where all resources are going to be created. * grid_storage_service : the type of storage used for tasks payloads, configurable between [S3 or Redis] * eks_worker : an array describing the autoscaling group used by EKS Deploying HTC-Grid \u00b6 The deployment time is about 30 min. Initialize terraform state for the grid make init-grid-deployment TAG = $TAG REGION = $HTCGRID_REGION All the dependencies have been created and are now ready. We are now ready to deploy the HTC-Grid project. There is one last thing to note. HTC-Grid deploys a Grafana version behind Amazon Cognito . While you can modify and select which passwords to use in Cognito, the Grafana internal deployment still requires an admin password. Select a memorable password change the value in the placeholder <my_grafana_admin_password> below (make this password follows Cognito default policy ): make apply-custom-runtime TAG = $TAG REGION = $HTCGRID_REGION GRAFANA_ADMIN_PASSWORD = <my_grafana_admin_password> Testing the deployment \u00b6 If make apply-custom-runtime is successful then in the deployment/grid/terraform folder two files are created: kubeconfig_htc_$TAG : this file give access to the EKS cluster through kubectl (example: kubeconfig_htc_aws_my_project) Agent_config.json : this file contains all the parameters, so the agent can run in the infrastructure Testing the Deployment Get the number of nodes in the cluster using the command below. Note: You should have one or more nodes. If not please the review the configuration files and particularly the variable eks_worker kubectl get nodes Check is system pods are running using the command below. Note: You should have all pods in running state (this might one minute but no more). kubectl -n kube-system get po Check if logging and monitoring is deployed using the command below. Note: You should have all pods in running state (this might one minute but no more). kubectl -n amazon-cloudwatch get po Check if metric server is deployed using the command below. Note: You should have all pods in running state (this might one minute but no more). kubectl -n custom-metrics get po Running the example workload \u00b6 In the folder mock_computation , you will find the code of the C++ program mocking computation. This program can sleep for a given duration or emulate CPU/memory consumption based on the input parameters. We will use a Kubernetes Jobs to submit one execution of 1 second of this C++ program. The communication between the job and the grid are implemented by a client in folder ./examples/client/python . Make sure the connection with the grid is established kubectl get nodes if an error is returned, please come back to step 2 of the previous section . Change directory to <project_root> Run the test: kubectl apply -f ./generated/single-task-test.yaml look at the log of the submission: kubectl logs job/single-task -f The test should take about 3 second to execute. If you see a successful message without exceptions raised, then the test has been successfully executed. clean the job submission instance: kubectl delete -f ./generated/single-task-test.yaml Create a Cognito user (CLI) \u00b6 All the services behind a public URL are protected with an authentication mechanism based on Cognito. So in order to access the Grafana dashboard you will need to create a Cognito user. Please from the root of the project : Choose a Cognito username: export USERNAME = <my_cognito_user> Choose a Cognito password (make this password follows Cognito default policy ) : You can reuse the password chosen in section Deploy HTC-Grid or create a new one. export PASSWORD = <my_grafana_admin_password> Get the client id: clientid = $( make get-client-id TAG = $TAG REGION = $HTCGRID_REGION ) Get the userpool id: userpoolid = $( make get-userpool-id TAG = $TAG REGION = $HTCGRID_REGION ) Create the user aws cognito-idp sign-up --region $HTCGRID_REGION --client-id $clientid --username $USERNAME --password $PASSWORD The user newly created will be unconfirmed. Confirm user creation: aws cognito-idp admin-confirm-sign-up --region $HTCGRID_REGION --user-pool-id $userpoolid --username $USERNAME Accessing Grafana \u00b6 The HTC-Grid project captures metrics into InfluxDB and exposes those metrics through Grafana. To secure Grafana To find out the HTTPS endpoint where Grafana has been deployed type: kubectl -n grafana get ingress | tail -n 1 | awk '{ print \"Grafana URL -> https://\"$4 }' It should output something like: Grafana URL -> https://k8s-grafana-grafanai-XXXXXXXXXXXX-YYYYYYYYYYY.eu-west-2.elb.amazonaws.com Then take the ADDRESS part and point at that on a browser. Note :It will generate a warning as we are using self-signed certificates. Just accept the self-signed certificate and you will be redirected to a Cognito sign in page. Please enter the username and password created in the previous section. Once you are sign up with Cognito you will be redirected to the Grafana sign in page. Please use the user admin and the password you selected at creation time to log in into Grafana. Uninstalling-Installing and destroying HTC grid \u00b6 The destruction time is about 15 min. To remove the grid resources run the following command: make destroy-custom-runtime TAG = $TAG REGION = $HTCGRID_REGION To remove the images from the ECR repository execute make destroy-images TAG = $TAG REGION = $HTCGRID_REGION Finally, this will leave the 3 only resources that you can clean manually, the S3 buckets. You can remove the folders using the following command make delete-grid-state TAG = $TAG REGION = $HTCGRID_REGION Build the documentation \u00b6 Go at the root of the git repository run the following command make doc or for deploying the server : make serve","title":"Happy Path"},{"location":"getting_started/happy-path/#happy-path","text":"","title":"Happy Path"},{"location":"getting_started/happy-path/#installing-the-htc-grid-software","text":"Unpack the provided HTC-Grid software ZIP (i.e: htc-grid-0.3.0.tar.gz ) or clone the repository into a local directory of your choice; this directory referred in this documentation as <project_root> . Unless stated otherwise, all paths referenced in this documentation are relative to <project_root> . For first time users or Windows users, we do recommend the use of Cloud9 as the platform to deploy HTC-Grid. The installation process uses Terraform and also make to build up artifacts and environment. This project provides a CloudFormation Cloud9 Stack that installs all the prerequisites-requisites listed above to deploy and develop HTC-Grid. Just follow the standard process in your account and deploy the Cloud9 CloudFormation Stack . Once the CloudFormation Stack has been created, open either the Output section in CloudFormation or go to Cloud9 in your AWS console and open the newly created Cloud9 environment.","title":"Installing the HTC-Grid software"},{"location":"getting_started/happy-path/#define-variables-for-deploying-the-infrastructure","text":"To simplify this installation it is suggested that a unique name (to be used later) is also used to prefix the different required bucket. TAG needs to follow S3 naming rules . export TAG = <Your tag> Define the region where the grid will be deployed export HTCGRID_REGION = <Your region> <Your region> region can be (the list is not exhaustive) eu-west-1 eu-west-2 eu-west-3 eu-central-1 us-east-1 us-west-2 ap-northeast-1 ap-southeast-1","title":"Define variables for deploying the infrastructure"},{"location":"getting_started/happy-path/#create-the-infrastructure-for-storing-the-state-of-the-htc-grid","text":"The following step creates 3 S3 buckets that will be needed during the installation: * 2 buckets will store the state of the different Terraform deployments * 1 bucket will store the HTC artifacts (the lambda to be executed by the agent) make init-grid-state TAG=$TAG REGION=$HTCGRID_REGION To validate the creation of the S3 buckets, you can run aws cloudformation describe-stacks --stack-name $TAG --region $HTCGRID_REGION --query 'Stacks[0]' That will list the 3 S3 Buckets that we just created.","title":"Create the infrastructure for storing the state of the HTC Grid"},{"location":"getting_started/happy-path/#create-and-deploy-htc-grid-images","text":"The HTC-Grid project has external software dependencies that are deployed as container images. Instead of downloading each time from the public DockerHub repository, this step will pull those dependencies and upload into the your Amazon Elastic Container Registry (ECR) . Important Note HTC-Grid uses a few open source project with container images stored at DockerHub . DockerHub has a download rate limit policy . This may impact you when running this step as an anonymous user as you can get errors when running the Terraform command below. To overcome those errors, you can re-run the Terraform command and wait until the throttling limit is lifted, or optionally you can create an account in hub.docker.com and then use the credentials of the account using docker login locally to avoid anonymous throttling limitations. As you'll be uploading images to ECR, to avoid timeouts, refresh your ECR authentication token: make ecr-login The following command will go to the ~/environment/aws-htc-grid/deployment/image_repository/terraform and initialize the Terraform project using the bucket $S3_IMAGE_TFSTATE_HTCGRID_BUCKET_NAME as the bucket that will hold the Terraform state: make init-images TAG=$TAG REGION=$HTCGRID_REGION If successful, you can now run terraform apply to create the HTC-Grid infrastructure. This can take between 10 and 15 minutes depending on the Internet connection. make transfer-images TAG = $TAG REGION = $HTCGRID_REGION The following command will list the repositories You can check which repositories have been created in the ECR console or by executing the command : aws ecr describe-repositories --query \"repositories[*].repositoryUri\" NB: This operation fetches images from external repositories and creates a copy into your ECR account, sometimes the fetch to external repositories may have temporary failures due to the state of the external repositories, If the terraform apply fails with errors such as the ones below, re-run the command until terraform apply successfully completes. name unknown: The repository with name 'xxxxxxxxx' does not exist in the registry with id","title":"Create and deploy HTC-Grid images"},{"location":"getting_started/happy-path/#build-htc-artifacts","text":"HTC artifacts include: python packages, docker images, configuration files for HTC and k8s. To build and install these: Now build the images for the HTC agent. Return to <project_root> and run the command: make happy-path TAG = $TAG REGION = $HTCGRID_REGION * If `TAG` is omitted then `mainline` will be the chosen has a default value . * If `REGION` is omitted then `eu-west-1` will be used . A folder name generated will be created at <project_root> . This folder should contain the following two files: * grid_config.json a configuration file for the grid with basic setting * single-task-test.yaml the Kubernetes configuration for running a single tasks on the grid.","title":"Build HTC artifacts"},{"location":"getting_started/happy-path/#configuring-the-htc-grid-runtime","text":"The grid_config.json is ready to deploy, but you can tune it before deployment. Some important parameters are: * region : the AWS region where all resources are going to be created. * grid_storage_service : the type of storage used for tasks payloads, configurable between [S3 or Redis] * eks_worker : an array describing the autoscaling group used by EKS","title":"Configuring the HTC-Grid runtime"},{"location":"getting_started/happy-path/#deploying-htc-grid","text":"The deployment time is about 30 min. Initialize terraform state for the grid make init-grid-deployment TAG = $TAG REGION = $HTCGRID_REGION All the dependencies have been created and are now ready. We are now ready to deploy the HTC-Grid project. There is one last thing to note. HTC-Grid deploys a Grafana version behind Amazon Cognito . While you can modify and select which passwords to use in Cognito, the Grafana internal deployment still requires an admin password. Select a memorable password change the value in the placeholder <my_grafana_admin_password> below (make this password follows Cognito default policy ): make apply-custom-runtime TAG = $TAG REGION = $HTCGRID_REGION GRAFANA_ADMIN_PASSWORD = <my_grafana_admin_password>","title":"Deploying HTC-Grid"},{"location":"getting_started/happy-path/#testing-the-deployment","text":"If make apply-custom-runtime is successful then in the deployment/grid/terraform folder two files are created: kubeconfig_htc_$TAG : this file give access to the EKS cluster through kubectl (example: kubeconfig_htc_aws_my_project) Agent_config.json : this file contains all the parameters, so the agent can run in the infrastructure Testing the Deployment Get the number of nodes in the cluster using the command below. Note: You should have one or more nodes. If not please the review the configuration files and particularly the variable eks_worker kubectl get nodes Check is system pods are running using the command below. Note: You should have all pods in running state (this might one minute but no more). kubectl -n kube-system get po Check if logging and monitoring is deployed using the command below. Note: You should have all pods in running state (this might one minute but no more). kubectl -n amazon-cloudwatch get po Check if metric server is deployed using the command below. Note: You should have all pods in running state (this might one minute but no more). kubectl -n custom-metrics get po","title":"Testing the deployment"},{"location":"getting_started/happy-path/#running-the-example-workload","text":"In the folder mock_computation , you will find the code of the C++ program mocking computation. This program can sleep for a given duration or emulate CPU/memory consumption based on the input parameters. We will use a Kubernetes Jobs to submit one execution of 1 second of this C++ program. The communication between the job and the grid are implemented by a client in folder ./examples/client/python . Make sure the connection with the grid is established kubectl get nodes if an error is returned, please come back to step 2 of the previous section . Change directory to <project_root> Run the test: kubectl apply -f ./generated/single-task-test.yaml look at the log of the submission: kubectl logs job/single-task -f The test should take about 3 second to execute. If you see a successful message without exceptions raised, then the test has been successfully executed. clean the job submission instance: kubectl delete -f ./generated/single-task-test.yaml","title":"Running the example workload"},{"location":"getting_started/happy-path/#create-a-cognito-user-cli","text":"All the services behind a public URL are protected with an authentication mechanism based on Cognito. So in order to access the Grafana dashboard you will need to create a Cognito user. Please from the root of the project : Choose a Cognito username: export USERNAME = <my_cognito_user> Choose a Cognito password (make this password follows Cognito default policy ) : You can reuse the password chosen in section Deploy HTC-Grid or create a new one. export PASSWORD = <my_grafana_admin_password> Get the client id: clientid = $( make get-client-id TAG = $TAG REGION = $HTCGRID_REGION ) Get the userpool id: userpoolid = $( make get-userpool-id TAG = $TAG REGION = $HTCGRID_REGION ) Create the user aws cognito-idp sign-up --region $HTCGRID_REGION --client-id $clientid --username $USERNAME --password $PASSWORD The user newly created will be unconfirmed. Confirm user creation: aws cognito-idp admin-confirm-sign-up --region $HTCGRID_REGION --user-pool-id $userpoolid --username $USERNAME","title":"Create a Cognito user (CLI)"},{"location":"getting_started/happy-path/#accessing-grafana","text":"The HTC-Grid project captures metrics into InfluxDB and exposes those metrics through Grafana. To secure Grafana To find out the HTTPS endpoint where Grafana has been deployed type: kubectl -n grafana get ingress | tail -n 1 | awk '{ print \"Grafana URL -> https://\"$4 }' It should output something like: Grafana URL -> https://k8s-grafana-grafanai-XXXXXXXXXXXX-YYYYYYYYYYY.eu-west-2.elb.amazonaws.com Then take the ADDRESS part and point at that on a browser. Note :It will generate a warning as we are using self-signed certificates. Just accept the self-signed certificate and you will be redirected to a Cognito sign in page. Please enter the username and password created in the previous section. Once you are sign up with Cognito you will be redirected to the Grafana sign in page. Please use the user admin and the password you selected at creation time to log in into Grafana.","title":"Accessing Grafana"},{"location":"getting_started/happy-path/#uninstalling-installing-and-destroying-htc-grid","text":"The destruction time is about 15 min. To remove the grid resources run the following command: make destroy-custom-runtime TAG = $TAG REGION = $HTCGRID_REGION To remove the images from the ECR repository execute make destroy-images TAG = $TAG REGION = $HTCGRID_REGION Finally, this will leave the 3 only resources that you can clean manually, the S3 buckets. You can remove the folders using the following command make delete-grid-state TAG = $TAG REGION = $HTCGRID_REGION","title":"Uninstalling-Installing and destroying HTC grid"},{"location":"getting_started/happy-path/#build-the-documentation","text":"Go at the root of the git repository run the following command make doc or for deploying the server : make serve","title":"Build the documentation"},{"location":"getting_started/portfolio_example/","text":"Portfolio Evaluation using QuantLib \u00b6 This example demonstrates the use of the QuantLib with its SWIG Python bindings . Components: - /examples/workloads/python/quant_lib/portfolio_pricing_client.py client application that generates a portfolio of trades using simple portfolio generator. Trades then being split into individual tasks and sent to the HTC-Grid for computation. Once all tasks are completed, the client application merges results together to determine total value of the portfolio. /examples/workloads/python/quant_lib/portfolio_pricing_engine.py compute engine that receives a list of trades to evaluate (could be entire portfolio or just a single trade). The engine uses QuantLib to evaluate the value of the portfolio. Deployment \u00b6 Follow all the steps in the main Readme file until you reach section \"Build HTC artifacts\" . In this section you will need to modify the make command replacing the happy-path with the python-quant-lib-path as follows: make python-quant-lib-path TAG = $TAG REGION = $HTCGRID_REGION This will apply the following changes: prepare python runtime environment for the lambda functions generate 2 sample yaml files that will be used to deploy testing client containers. After make is completed, please run make apply-python-runtime TAG = $TAG REGION = $HTCGRID_REGION Follow all the remaining steps as is in the main readme file. Running the example \u00b6 Two default configurations are provided. The first configuration submits a portfolio containing a single trade. kubectl apply -f ./generated/portfolio-pricing-single-trade.yaml The second configuration submits a portfolio containing multiple trades. kubectl apply -f ./generated/portfolio-pricing-book.yaml Refer to the corresponding yaml files to change the configuration of the client application and refer to the help of the client application to identify all options. Uninstalling-Installing and destroying HTC grid \u00b6 The destruction time is about 15 min. To remove the grid resources run the following command: make destroy-python-runtime TAG = $TAG REGION = $HTCGRID_REGION To remove the images from the ECR repository execute make destroy-images TAG = $TAG REGION = $HTCGRID_REGION Finally, this will leave the 3 only resources that you can clean manually, the S3 buckets. You can remove the folders using the following command make delete-grid-state TAG = $TAG REGION = $HTCGRID_REGION","title":"Portfolio example"},{"location":"getting_started/portfolio_example/#portfolio-evaluation-using-quantlib","text":"This example demonstrates the use of the QuantLib with its SWIG Python bindings . Components: - /examples/workloads/python/quant_lib/portfolio_pricing_client.py client application that generates a portfolio of trades using simple portfolio generator. Trades then being split into individual tasks and sent to the HTC-Grid for computation. Once all tasks are completed, the client application merges results together to determine total value of the portfolio. /examples/workloads/python/quant_lib/portfolio_pricing_engine.py compute engine that receives a list of trades to evaluate (could be entire portfolio or just a single trade). The engine uses QuantLib to evaluate the value of the portfolio.","title":"Portfolio Evaluation using QuantLib"},{"location":"getting_started/portfolio_example/#deployment","text":"Follow all the steps in the main Readme file until you reach section \"Build HTC artifacts\" . In this section you will need to modify the make command replacing the happy-path with the python-quant-lib-path as follows: make python-quant-lib-path TAG = $TAG REGION = $HTCGRID_REGION This will apply the following changes: prepare python runtime environment for the lambda functions generate 2 sample yaml files that will be used to deploy testing client containers. After make is completed, please run make apply-python-runtime TAG = $TAG REGION = $HTCGRID_REGION Follow all the remaining steps as is in the main readme file.","title":"Deployment"},{"location":"getting_started/portfolio_example/#running-the-example","text":"Two default configurations are provided. The first configuration submits a portfolio containing a single trade. kubectl apply -f ./generated/portfolio-pricing-single-trade.yaml The second configuration submits a portfolio containing multiple trades. kubectl apply -f ./generated/portfolio-pricing-book.yaml Refer to the corresponding yaml files to change the configuration of the client application and refer to the help of the client application to identify all options.","title":"Running the example"},{"location":"getting_started/portfolio_example/#uninstalling-installing-and-destroying-htc-grid","text":"The destruction time is about 15 min. To remove the grid resources run the following command: make destroy-python-runtime TAG = $TAG REGION = $HTCGRID_REGION To remove the images from the ECR repository execute make destroy-images TAG = $TAG REGION = $HTCGRID_REGION Finally, this will leave the 3 only resources that you can clean manually, the S3 buckets. You can remove the folders using the following command make delete-grid-state TAG = $TAG REGION = $HTCGRID_REGION","title":"Uninstalling-Installing and destroying HTC grid"},{"location":"getting_started/prerequisite/","text":"Software Prerequisites \u00b6 The following resources should be installed upon you local machine (Linux and macOS only are supported). docker version > 1.19 kubectl version > 1.19 (usually installed alongside Docker) python 3.7 virtualenv AWS CLI version 2 terraform v0.13.4 or terraform v0.14.9 helm version > 3 JQ","title":"Software Prerequisites"},{"location":"getting_started/prerequisite/#software-prerequisites","text":"The following resources should be installed upon you local machine (Linux and macOS only are supported). docker version > 1.19 kubectl version > 1.19 (usually installed alongside Docker) python 3.7 virtualenv AWS CLI version 2 terraform v0.13.4 or terraform v0.14.9 helm version > 3 JQ","title":"Software Prerequisites"},{"location":"user_guide/configuring_priority_queues/","text":"Configuring Priority Queues \u00b6 By default HTC-Grid comes configured with a single task queue implemented via SQS. However, HTC-Grid also supports task prioritization which is implemented suing multiple SQS queues. In such configuration each SQS queue corresponds to a specific priority. At runtime Agents attempt to pull tasks from the higher priority queues before checking queues containing lower priority. To enable multiple priorities the following 3 steps need to be configured prior to HTC-Grid deployment in ''deployment/grid/terraform/control_plane/sqs.tf'' modify variable priorities to have sufficient number of priorities that are required. Follow the same naming/numbering convention as outlined below # Default configuration with 1 priority variable \"priorities\" { default = { \"__0\" = 0 } } ... # Example configuration with 3 priorities variable \"priorities\" { default = { \"__0\" = 0 \"__1\" = 1 \"__2\" = 2 } } Configure GRID_CONFIG file (e.g., ''python_runtime_grid_config.json'') before deploying HTC-Grid. Note this file is auto-generated from the corresponding .tpl file located in ''examples/configurations/'' hence re-running ''make'' can overwrite modifications, consider updating .tpl file instead. \"task_queue_service\" : \"PrioritySQS\" , \"task_queue_config\" : \"{'priorities':3}\" , Set ''task_queue_service'' to PrioritySQS indicating that multiple priorities are used. Then, update ''task_queue_config'' to contain the appropriate number of priorities created in step 1.","title":"Configuring Priority Queues"},{"location":"user_guide/configuring_priority_queues/#configuring-priority-queues","text":"By default HTC-Grid comes configured with a single task queue implemented via SQS. However, HTC-Grid also supports task prioritization which is implemented suing multiple SQS queues. In such configuration each SQS queue corresponds to a specific priority. At runtime Agents attempt to pull tasks from the higher priority queues before checking queues containing lower priority. To enable multiple priorities the following 3 steps need to be configured prior to HTC-Grid deployment in ''deployment/grid/terraform/control_plane/sqs.tf'' modify variable priorities to have sufficient number of priorities that are required. Follow the same naming/numbering convention as outlined below # Default configuration with 1 priority variable \"priorities\" { default = { \"__0\" = 0 } } ... # Example configuration with 3 priorities variable \"priorities\" { default = { \"__0\" = 0 \"__1\" = 1 \"__2\" = 2 } } Configure GRID_CONFIG file (e.g., ''python_runtime_grid_config.json'') before deploying HTC-Grid. Note this file is auto-generated from the corresponding .tpl file located in ''examples/configurations/'' hence re-running ''make'' can overwrite modifications, consider updating .tpl file instead. \"task_queue_service\" : \"PrioritySQS\" , \"task_queue_config\" : \"{'priorities':3}\" , Set ''task_queue_service'' to PrioritySQS indicating that multiple priorities are used. Then, update ''task_queue_config'' to contain the appropriate number of priorities created in step 1.","title":"Configuring Priority Queues"},{"location":"user_guide/creating_your_a_client/","text":"Configuring Client and Sending Tasks to the HTC Grid \u00b6 We assume that all previous steps have been successfully completed, there is at least one pod that is running in the system and thus can execute tasks. Furthermore, we consider that client application will be running on an EC2 instance. Setup a Cloud9 (EC2) Instance a) Go to the AWS console in the Cloud9 service b) Click on \"Create your environment\" c) Select your favorite OS and machine type (at least t3.small) d) In network settings, please used the VPC id and, the subnet that was created as part of your infrastructure deployment (look for your unique suffix in the VPC/subnet pages of the AWS console). Go to the AWS console in the VPC service. Find the VPC id where the EKS cluster Find a public subnet id attached to the VPC where the EKS cluster is running. (The public subnet should have an Internet Gateways and should be able to assign IPs) e) Click on create and wait for the validation Checkout the same version of the repository as was used to deploy infrastructure Upload the infrastructure settings to the Cloud9 instance so that client knows ALB endpoints and connects to the right instance of the HTC grid. a) On the machine that was used for deployment. Go in ./infrastructure/ folder where terraform apply has been run. terraform output agent_config c) copy the produced Agent_config.json file into cloud9 and note the location d) set environment variable export AGENT_CONFIG_FILE=/<path>/Agent_config.json e) set environment variable export INTRA_VPC=1 This will allow client to send tasks to ALB without authentication through Cognito as it is deployed in the same VPC as the grid. For clients running from outside the VPC an additional authentication step is required. From the root folder execute: make packages #make sure that these two files are created: ls ./dist/ api-0.1-py3-none-any.whl utils-0.1-py3-none-any.whl If these files are not created or if it is based on python 2, then run virtualenv as described at the start of the document Install clients requirements on Cloud9 cd ./examples/client/python/ pip3 install -r requirements.txt Sample client and workload generator is located here ./examples/client/python/client.py . Read help and browse through the code to be able to submit tasks and sessions, see examples below: To show the example client application help python3 ./client.py --help To submits a single session (containing a single task by default) python3 ./client.py --njobs 1 To submits 2 batches of 4 sessions each, where each session contains 3 tasks. Total 4*3*2 24 tasks. python3 ./client.py --njobs 2 --job_size 3 --job_batch_size 4 To tarts 5 threads, each submits a single session with 1 job with a custom arguments to the executable. python3 ./client.py --njobs 1 --worker_arguments \"5000 1 100\" -nthreads 5","title":"Configuring Client and Sending Tasks to the HTC Grid"},{"location":"user_guide/creating_your_a_client/#configuring-client-and-sending-tasks-to-the-htc-grid","text":"We assume that all previous steps have been successfully completed, there is at least one pod that is running in the system and thus can execute tasks. Furthermore, we consider that client application will be running on an EC2 instance. Setup a Cloud9 (EC2) Instance a) Go to the AWS console in the Cloud9 service b) Click on \"Create your environment\" c) Select your favorite OS and machine type (at least t3.small) d) In network settings, please used the VPC id and, the subnet that was created as part of your infrastructure deployment (look for your unique suffix in the VPC/subnet pages of the AWS console). Go to the AWS console in the VPC service. Find the VPC id where the EKS cluster Find a public subnet id attached to the VPC where the EKS cluster is running. (The public subnet should have an Internet Gateways and should be able to assign IPs) e) Click on create and wait for the validation Checkout the same version of the repository as was used to deploy infrastructure Upload the infrastructure settings to the Cloud9 instance so that client knows ALB endpoints and connects to the right instance of the HTC grid. a) On the machine that was used for deployment. Go in ./infrastructure/ folder where terraform apply has been run. terraform output agent_config c) copy the produced Agent_config.json file into cloud9 and note the location d) set environment variable export AGENT_CONFIG_FILE=/<path>/Agent_config.json e) set environment variable export INTRA_VPC=1 This will allow client to send tasks to ALB without authentication through Cognito as it is deployed in the same VPC as the grid. For clients running from outside the VPC an additional authentication step is required. From the root folder execute: make packages #make sure that these two files are created: ls ./dist/ api-0.1-py3-none-any.whl utils-0.1-py3-none-any.whl If these files are not created or if it is based on python 2, then run virtualenv as described at the start of the document Install clients requirements on Cloud9 cd ./examples/client/python/ pip3 install -r requirements.txt Sample client and workload generator is located here ./examples/client/python/client.py . Read help and browse through the code to be able to submit tasks and sessions, see examples below: To show the example client application help python3 ./client.py --help To submits a single session (containing a single task by default) python3 ./client.py --njobs 1 To submits 2 batches of 4 sessions each, where each session contains 3 tasks. Total 4*3*2 24 tasks. python3 ./client.py --njobs 2 --job_size 3 --job_batch_size 4 To tarts 5 threads, each submits a single session with 1 job with a custom arguments to the executable. python3 ./client.py --njobs 1 --worker_arguments \"5000 1 100\" -nthreads 5","title":"Configuring Client and Sending Tasks to the HTC Grid"},{"location":"user_guide/htcgrid_management/","text":"Managing HTC-Grid after deployment \u00b6 Managing the HTC-Grid agent \u00b6 Make connection with EKS, go in the deployment/grid/terraform folder where terraform apply has been run. Then make sure that KUBECONFIG is set (see step 6). Set up the storage for helm export HELM_DRIVER = configmap Deploy the agent helm install <your release name> ../charts/agent-htc-lambda --set fullnameOverride = htc-agent Test the installation by running helm test <your release name> Delete the agent helm uninstall <your release name> Get deployed pods ( see notes after agent deployment ) Get the log of a pod: ( see notes after agent deployment ) Describe the state of a pod , useful for debugging situation never run, (see notes after agent deployment) Execute a command into a pod kubectl exec <pod name> -c <container name> <command> Open an interactive session into a pod kubectl exec -it <pod name> bash Get config-map used in the pods, i.e., current configuration kubectl get cm agent-configmap -o yaml Get information about the pod autoscaler kubectl get hpa kubectl get hpa -w Updating number of replicas (my_cluster is your release name that you selected with helm) helm upgrade --set replicaCount=5 --set foo=newbar my_cluster ./agent-htc-lambda Common commands \u00b6 Get logs from a running agent and lambda worker kubectl logs -c <agent or lambda> <pod name> Examples: kubectl logs -c agent htc-agent-544bd95456-wgzqs kubectl logs -c lambda htc-agent-544bd95456-wgzqs Launching Tests cd root/examples/submissions/k8s_jobs kubectl apply -f <test-name.yaml> example: kubectl apply -f scaling-test.yaml Follow the execution of the test kubectl logs job/scaling-test -f Deleting all running pods kubectl delete $( kubectl get po -o name )","title":"Managing HTC-Grid after deployment"},{"location":"user_guide/htcgrid_management/#managing-htc-grid-after-deployment","text":"","title":"Managing HTC-Grid after deployment"},{"location":"user_guide/htcgrid_management/#managing-the-htc-grid-agent","text":"Make connection with EKS, go in the deployment/grid/terraform folder where terraform apply has been run. Then make sure that KUBECONFIG is set (see step 6). Set up the storage for helm export HELM_DRIVER = configmap Deploy the agent helm install <your release name> ../charts/agent-htc-lambda --set fullnameOverride = htc-agent Test the installation by running helm test <your release name> Delete the agent helm uninstall <your release name> Get deployed pods ( see notes after agent deployment ) Get the log of a pod: ( see notes after agent deployment ) Describe the state of a pod , useful for debugging situation never run, (see notes after agent deployment) Execute a command into a pod kubectl exec <pod name> -c <container name> <command> Open an interactive session into a pod kubectl exec -it <pod name> bash Get config-map used in the pods, i.e., current configuration kubectl get cm agent-configmap -o yaml Get information about the pod autoscaler kubectl get hpa kubectl get hpa -w Updating number of replicas (my_cluster is your release name that you selected with helm) helm upgrade --set replicaCount=5 --set foo=newbar my_cluster ./agent-htc-lambda","title":"Managing the HTC-Grid agent"},{"location":"user_guide/htcgrid_management/#common-commands","text":"Get logs from a running agent and lambda worker kubectl logs -c <agent or lambda> <pod name> Examples: kubectl logs -c agent htc-agent-544bd95456-wgzqs kubectl logs -c lambda htc-agent-544bd95456-wgzqs Launching Tests cd root/examples/submissions/k8s_jobs kubectl apply -f <test-name.yaml> example: kubectl apply -f scaling-test.yaml Follow the execution of the test kubectl logs job/scaling-test -f Deleting all running pods kubectl delete $( kubectl get po -o name )","title":"Common commands"},{"location":"user_guide/troubleshooting/","text":"Troubleshooting HTC-Grid \u00b6 This section captures some of the errors we have captured in the past and how to resolve them Error on terraform apply: \u00b6 terraform apply -var-file ./../src/eks/Agent_config_mainline.json\" ... ... Error: DeleteConflict: Certificate: XXXXXXXXXXX is currently in use by arn:aws:elasticloadbalancing:eu-west-1:XXXXXXXXX:loadbalancer/app/k8s-grafana-grafanai-026d965437/c04519ef28804b31. Please remove it first before deleting it from IAM. status code: 409, request id: 9e621094-2dba-44ac-967d-c764470c1474 Resolution: \u00b6 kubectl -n grafana get ingress kubectl -n grafana delete ingress grafana-ingress terraform apply -var-file ./../src/eks/Agent_config_mainline.json\" Error from terraform Apply when pulling the images \u00b6 Error: Error running command 'if ! docker pull curlimages/curl:7.73.0 then echo \"cannot download image curlimages/curl:7.73.0\" exit 1 fi if ! docker tag curlimages/curl:7.73.0 300962108239.dkr.ecr.eu-west-1.amazonaws.com/curl:7.73.0 then echo \"cannot tag curlimages/curl:7.73.0 to 300962108239.dkr.ecr.eu-west-1.amazonaws.com/curl:7.73.0\" exit 1 fi if ! docker push 300962108239.dkr.ecr.eu-west-1.amazonaws.com/curl:7.73.0 then echo \"echo cannot push 300962108239.dkr.ecr.eu-west-1.amazonaws.com/curl:7.73.0\" exit 1 fi ': exit status 1. Output: 7.73.0: Pulling from curlimages/curl Resolution \u00b6 Rerun the terraform command, DockerHub has throttling limits that may cause spurious errors like this Error on terraform apply: \u00b6 Error: cannot re-use a name that is still in use on resources/influxd.tf line 19, in resource \"helm_release\" \"influxdb\": 19: resource \"helm_release\" \"influxdb\" { Resolution: \u00b6 export HELM_DRIVER=configmap helm list -n influxdb helm -n influxdb uninstall influxdb ... <restart tarraform apply> Error on terraform apply: \u00b6 Error: error reading VPC Endpoint Service (com.amazonaws.eu-north-1.elasticloadbalancing): InvalidServiceName: The Vpc Endpoint Service 'com.amazonaws.eu-north-1.elasticloadbalancing' does not exist status code: 400, request id: 60127863-944c-467b-983b-8f8b79f332c0 on .terraform/modules/vpc.vpc/vpc-endpoints.tf line 656, in data \"aws_vpc_endpoint_service\" \"elasticloadbalancing\": 656: data \"aws_vpc_endpoint_service\" \"elasticloadbalancing\" { Resolution \u00b6 Some AWS regions currently don't have VPC Endpoint Services available for certain services used by HTC-Grid. This means that at this stage HTC-Grid can not be deployed in these regions. Below is the list of tested regions where we encountered this issue: * eu-north-1","title":"Troubleshooting"},{"location":"user_guide/troubleshooting/#troubleshooting-htc-grid","text":"This section captures some of the errors we have captured in the past and how to resolve them","title":"Troubleshooting HTC-Grid"},{"location":"user_guide/troubleshooting/#error-on-terraform-apply","text":"terraform apply -var-file ./../src/eks/Agent_config_mainline.json\" ... ... Error: DeleteConflict: Certificate: XXXXXXXXXXX is currently in use by arn:aws:elasticloadbalancing:eu-west-1:XXXXXXXXX:loadbalancer/app/k8s-grafana-grafanai-026d965437/c04519ef28804b31. Please remove it first before deleting it from IAM. status code: 409, request id: 9e621094-2dba-44ac-967d-c764470c1474","title":"Error on terraform apply:"},{"location":"user_guide/troubleshooting/#resolution","text":"kubectl -n grafana get ingress kubectl -n grafana delete ingress grafana-ingress terraform apply -var-file ./../src/eks/Agent_config_mainline.json\"","title":"Resolution:"},{"location":"user_guide/troubleshooting/#error-from-terraform-apply-when-pulling-the-images","text":"Error: Error running command 'if ! docker pull curlimages/curl:7.73.0 then echo \"cannot download image curlimages/curl:7.73.0\" exit 1 fi if ! docker tag curlimages/curl:7.73.0 300962108239.dkr.ecr.eu-west-1.amazonaws.com/curl:7.73.0 then echo \"cannot tag curlimages/curl:7.73.0 to 300962108239.dkr.ecr.eu-west-1.amazonaws.com/curl:7.73.0\" exit 1 fi if ! docker push 300962108239.dkr.ecr.eu-west-1.amazonaws.com/curl:7.73.0 then echo \"echo cannot push 300962108239.dkr.ecr.eu-west-1.amazonaws.com/curl:7.73.0\" exit 1 fi ': exit status 1. Output: 7.73.0: Pulling from curlimages/curl","title":"Error from terraform Apply when pulling the images"},{"location":"user_guide/troubleshooting/#resolution_1","text":"Rerun the terraform command, DockerHub has throttling limits that may cause spurious errors like this","title":"Resolution"},{"location":"user_guide/troubleshooting/#error-on-terraform-apply_1","text":"Error: cannot re-use a name that is still in use on resources/influxd.tf line 19, in resource \"helm_release\" \"influxdb\": 19: resource \"helm_release\" \"influxdb\" {","title":"Error on terraform apply:"},{"location":"user_guide/troubleshooting/#resolution_2","text":"export HELM_DRIVER=configmap helm list -n influxdb helm -n influxdb uninstall influxdb ... <restart tarraform apply>","title":"Resolution:"},{"location":"user_guide/troubleshooting/#error-on-terraform-apply_2","text":"Error: error reading VPC Endpoint Service (com.amazonaws.eu-north-1.elasticloadbalancing): InvalidServiceName: The Vpc Endpoint Service 'com.amazonaws.eu-north-1.elasticloadbalancing' does not exist status code: 400, request id: 60127863-944c-467b-983b-8f8b79f332c0 on .terraform/modules/vpc.vpc/vpc-endpoints.tf line 656, in data \"aws_vpc_endpoint_service\" \"elasticloadbalancing\": 656: data \"aws_vpc_endpoint_service\" \"elasticloadbalancing\" {","title":"Error on terraform apply:"},{"location":"user_guide/troubleshooting/#resolution_3","text":"Some AWS regions currently don't have VPC Endpoint Services available for certain services used by HTC-Grid. This means that at this stage HTC-Grid can not be deployed in these regions. Below is the list of tested regions where we encountered this issue: * eu-north-1","title":"Resolution"}]}